from cartpole.evaluate import evaluate_policy_1, evaluate_trained_policy
from cartpole.policy2_reinforce import train_policy_network
from cartpole.plot import plot_returns
from qlearning.run_qlearning import run_all_qlearning, print_results
import numpy as np

def main():
    print("\n" + "="*60)
    print("TRABALHO 5: APRENDIZADO POR REFOR√áO")
    print("="*60)

    # ---------------------
    # Parte 1 ‚Äî CartPole
    # ---------------------
    print("\n" + "="*40)
    print("PARTE 1: PROBLEMA DO CARTPOLE")
    print("="*40)
    
    print("\nComparando duas pol√≠ticas para manter a haste na vertical:")
    print("- Pol√≠tica 1: Acelera para esquerda/direita baseado na inclina√ß√£o do poste")
    print("- Pol√≠tica 2: Rede neural treinada com algoritmo REINFORCE")
    print("\nCada pol√≠tica ser√° avaliada em 10 epis√≥dios de at√© 500 tentativas.")
    
    # Avalia Pol√≠tica 1
    print("\n" + "-"*30)
    policy1_returns = evaluate_policy_1(episodes=10, max_steps=500)
    
    print("\nüìä RESULTADOS - POL√çTICA 1 (Determin√≠stica):")
    print(f"   M√©dia: {np.mean(policy1_returns):.2f} tentativas")
    print(f"   Desvio padr√£o: {np.std(policy1_returns):.2f}")
    print(f"   Valores: {policy1_returns}")
    
    # Treina e avalia Pol√≠tica 2
    print("\n" + "-"*30)
    policy2, training_returns = train_policy_network(episodes=10, max_steps=500, use_gpu=True)
    
    # Avalia a pol√≠tica treinada separadamente
    policy2_returns = evaluate_trained_policy(policy2, episodes=10, max_steps=500)
    
    print("\nüìä RESULTADOS - POL√çTICA 2 (REINFORCE):")
    print(f"   M√©dia: {np.mean(policy2_returns):.2f} tentativas")
    print(f"   Desvio padr√£o: {np.std(policy2_returns):.2f}")
    print(f"   Valores: {policy2_returns}")
    
    # Compara√ß√£o
    print("\nüìà COMPARA√á√ÉO:")
    if np.mean(policy1_returns) > np.mean(policy2_returns):
        print("   ‚Üí Pol√≠tica 1 obteve melhor desempenho m√©dio")
    else:
        print("   ‚Üí Pol√≠tica 2 obteve melhor desempenho m√©dio")
    
    print(f"   ‚Üí Diferen√ßa na m√©dia: {abs(np.mean(policy1_returns) - np.mean(policy2_returns)):.2f} tentativas")

    # Gera gr√°fico
    plot_returns(policy1_returns, policy2_returns)

    # ---------------------
    # Parte 2 ‚Äî Q-Learning
    # ---------------------
    print("\n" + "="*40)
    print("PARTE 2: PROBLEMA DO ROB√î (Q-LEARNING)")
    print("="*40)
    
    print("\nProblema: Rob√¥ de reciclagem que coleta latas com bateria recarreg√°vel")
    print("Estados: high (bateria alta), low (bateria baixa)")  
    print("A√ß√µes: search (procurar latas), wait (esperar), recharge (recarregar)")
    print("\nEstimando a√ß√£o √≥tima para cada estado usando Q-Learning.")
    print("Simulando tr√™s configura√ß√µes de par√¢metros:")
    print("a) Œ±=0.2 (learning rate), Œ≤=0.2 (discount factor)")
    print("b) Œ±=0.4 (learning rate), Œ≤=0.1 (discount factor)") 
    print("c) Œ±=0.1 (learning rate), Œ≤=0.4 (discount factor)")

    results = run_all_qlearning()
    print_results(results)
    
    print("\n" + "="*60)
    print("TRABALHO CONCLU√çDO COM SUCESSO!")
    print("="*60)

if __name__ == "__main__":
    main()
