# Trabalho 5: Aprendizado por ReforÃ§o

**Autor:** Diego Giovanni de AlcÃ¢ntara Vieira  
**Programa:** PÃ³s-GraduaÃ§Ã£o em Engenharia ElÃ©trica - UFAM  
**Email:** diego.vieira@ufam.edu.br

[![Python](https://img.shields.io/badge/Python-3.12+-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.9+-red.svg)](https://pytorch.org)
[![CUDA](https://img.shields.io/badge/CUDA-13.0-green.svg)](https://developer.nvidia.com/cuda-toolkit)
[![License](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

## ğŸ“‹ Resumo Executivo

Este projeto implementa e compara duas abordagens fundamentais de **Aprendizado por ReforÃ§o** em problemas clÃ¡ssicos:

### ğŸ¯ **Parte 1 - CartPole**
ComparaÃ§Ã£o entre polÃ­tica determinÃ­stica e rede neural treinada com **REINFORCE** (algoritmo de Ronald Williams) com suporte completo a **GPU (CUDA)**. 

**Resultados:** PolÃ­tica determinÃ­stica superior (**40,4Â±7,9** vs **9,5Â±0,9** tentativas)

### ğŸ¤– **Parte 2 - RobÃ´ de Reciclagem**  
AplicaÃ§Ã£o de **Q-Learning** em MDP discreto com trÃªs configuraÃ§Ãµes de hiperparÃ¢metros, revelando polÃ­tica Ã³tima consistente: buscar latas em bateria alta, aguardar em bateria baixa.

---

## ğŸ—ï¸ Estrutura do Projeto

```
reinforcement-learning/
â”œâ”€â”€ main.py                    # ğŸš€ ExecuÃ§Ã£o principal
â”œâ”€â”€ main.tex                   # ğŸ“„ RelatÃ³rio cientÃ­fico LaTeX
â”œâ”€â”€ cartpole/                  # ğŸ® Parte 1: Problema CartPole
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ policy1.py            # PolÃ­tica determinÃ­stica
â”‚   â”œâ”€â”€ policy2_reinforce.py  # PolÃ­tica REINFORCE + GPU
â”‚   â”œâ”€â”€ evaluate.py           # AvaliaÃ§Ã£o de polÃ­ticas
â”‚   â””â”€â”€ plot.py               # VisualizaÃ§Ã£o de resultados
â”œâ”€â”€ qlearning/                # ğŸ§  Parte 2: Q-Learning
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mdp.py               # DefiniÃ§Ã£o do MDP do robÃ´
â”‚   â”œâ”€â”€ qlearning.py         # Algoritmo Q-Learning
â”‚   â””â”€â”€ run_qlearning.py     # ExecuÃ§Ã£o dos cenÃ¡rios
â”œâ”€â”€ test_gpu.py              # âš¡ Teste de performance GPU vs CPU
â””â”€â”€ returns_plot.png         # ğŸ“Š GrÃ¡fico comparativo gerado
```

---

## ğŸš€ InÃ­cio RÃ¡pido

### PrÃ©-requisitos

```bash
# DependÃªncias bÃ¡sicas
pip install gymnasium torch numpy matplotlib

# Para GPU (opcional)
# NVIDIA GPU + CUDA drivers + PyTorch CUDA
```

**Verificar GPU:**
```bash
python -c "import torch; print(f'CUDA disponÃ­vel: {torch.cuda.is_available()}')"
```

### ExecuÃ§Ã£o

```bash
# ExecuÃ§Ã£o completa (GPU automÃ¡tico)
python main.py

# Teste de performance GPU vs CPU
python test_gpu.py
```

### Controle de GPU

```python
# ForÃ§ar CPU
train_policy_network(use_gpu=False)

# ForÃ§ar GPU (se disponÃ­vel)  
train_policy_network(use_gpu=True)
```

---

## ğŸ“Š Resultados Experimentais

### Parte 1: CartPole

| **PolÃ­tica** | **MÃ©dia** | **Desvio** | **Dispositivo** |
|--------------|-----------|------------|-----------------|
| DeterminÃ­stica | 40,4 | 7,9 | CPU |
| REINFORCE | 9,5 | 0,9 | GPU (RTX 4050) |

**Performance GPU:**
- **Dispositivo:** NVIDIA GeForce RTX 4050 Laptop GPU
- **Tempo:** 0,34s de treinamento  
- **MemÃ³ria:** 0,02 GB VRAM utilizada
- **CUDA:** 13.0

### Parte 2: Q-Learning

#### CenÃ¡rio A: Î±=0.2, Î²=0.2

| **Estado** | **search** | **wait** | **recharge** |
|------------|------------|----------|--------------|
| high | 2,500 | 1,500 | N/A |
| low | -1,000 | 1,500 | 0,500 |

**PolÃ­tica Ã“tima:** highâ†’search, lowâ†’wait

#### CenÃ¡rio B: Î±=0.4, Î²=0.1

| **Estado** | **search** | **wait** | **recharge** |
|------------|------------|----------|--------------|
| high | 2,222 | 1,222 | N/A |
| low | -1,278 | 1,222 | 0,222 |

**PolÃ­tica Ã“tima:** highâ†’search, lowâ†’wait

#### CenÃ¡rio C: Î±=0.1, Î²=0.4

| **Estado** | **search** | **wait** | **recharge** |
|------------|------------|----------|--------------|
| high | 3,333 | 2,333 | N/A |
| low | -0,167 | 2,333 | 1,333 |

**PolÃ­tica Ã“tima:** highâ†’search, lowâ†’wait

---

## ğŸ”¬ Metodologia CientÃ­fica

### Parte 1: CartPole

#### PolÃ­tica 1 (DeterminÃ­stica)
EstratÃ©gia baseada na inclinaÃ§Ã£o da haste:
```python
aÃ§Ã£o = 1 se Î¸ > 0 (direita)
aÃ§Ã£o = 0 se Î¸ â‰¤ 0 (esquerda)
```

#### PolÃ­tica 2 (REINFORCE)
**Arquitetura da Rede Neural:**
- **Entrada:** 4 observaÃ§Ãµes (posiÃ§Ã£o, velocidade, Ã¢ngulo, velocidade angular)
- **Camadas Ocultas:** 2 Ã— 24 neurÃ´nios + ReLU
- **SaÃ­da:** 2 aÃ§Ãµes + Softmax (distribuiÃ§Ã£o de probabilidades)

**Algoritmo REINFORCE:**
```
âˆ‡Î¸ J(Î¸) = E[âˆ‘(t=0 to T) âˆ‡Î¸ log Ï€Î¸(at|st) Â· Gt]
Gt = âˆ‘(k=t to T) Î³^(k-t) Â· rk  (Î³ = 0.99)
```

**ImplementaÃ§Ã£o GPU:**
- âœ… DetecÃ§Ã£o automÃ¡tica CUDA
- âœ… TransferÃªncia eficiente de tensores  
- âœ… Monitoramento de memÃ³ria
- âœ… Gradient clipping para estabilidade

### Parte 2: MDP do RobÃ´

**Estados:** `{high, low}` (nÃ­veis de bateria)  
**AÃ§Ãµes:** `{search, wait, recharge}` (recharge sÃ³ em low)

**DinÃ¢mica de Recompensas:**
- `search`: R=+2.0 (coleta ativa, risco bateria)
- `wait`: R=+1.0 (coleta passiva, economia energia)
- Esgotamento: R=-3.0 (penalidade resgate)

**Q-Learning:**
```
Q(s,a) â† Q(s,a) + Î±[r + Î²Â·max Q(s',a') - Q(s,a)]
```

**CenÃ¡rios:** 
- a) Î±=0.2, Î²=0.2 (balanceado)
- b) Î±=0.4, Î²=0.1 (rÃ¡pido, presente) 
- c) Î±=0.1, Î²=0.4 (lento, futuro)

---

## ğŸ¯ AnÃ¡lise e DiscussÃ£o

### Por que PolÃ­tica DeterminÃ­stica Venceu?

1. **Conhecimento do DomÃ­nio:** Aproveita fÃ­sica do sistema diretamente
2. **InsuficiÃªncia de Dados:** 10 episÃ³dios inadequados para rede neural
3. **Complexidade vs Necessidade:** Problema simples nÃ£o justifica deep learning

### Quando REINFORCE Seria Superior?

- Problemas com dinÃ¢micas complexas/nÃ£o-lineares
- Estados parcialmente observÃ¡veis  
- Ambientes com ruÃ­do significativo
- Treinamento extenso (>100 episÃ³dios)

### Robustez do Q-Learning

- **PolÃ­tica Ã“tima Consistente:** Independente dos hiperparÃ¢metros testados
- **Î± (learning rate):** Afeta velocidade, nÃ£o altera soluÃ§Ã£o final
- **Î² (discount factor):** Influencia magnitude Q, preserva ordenaÃ§Ã£o
- **MDP Bem-Estruturado:** SoluÃ§Ã£o Ã³tima clara e estÃ¡vel

---

## ğŸš€ ImplementaÃ§Ã£o GPU

### Funcionalidades

âœ… **DetecÃ§Ã£o AutomÃ¡tica:** GPU/CPU transparente  
âœ… **OtimizaÃ§Ãµes:** TransferÃªncias eficientes, cache management  
âœ… **Monitoramento:** Uso de memÃ³ria e performance  
âœ… **Compatibilidade:** Funciona com/sem GPU  

### Sistema Testado

- **GPU:** NVIDIA GeForce RTX 4050 Laptop GPU (6.1 GB)
- **CUDA:** 13.0  
- **Framework:** PyTorch 2.9.1+cu130

### Casos Ideais para GPU

- Redes neurais grandes (>1M parÃ¢metros)
- Treinamento longo (>100 episÃ³dios)  
- MÃºltiplos ambientes paralelos
- Experimentos de hiperparÃ¢metros

---

## ğŸ“ˆ Resultados e ConclusÃµes

### Principais Descobertas

#### CartPole
- **PolÃ­tica determinÃ­stica superou rede neural** (diferenÃ§a: 30,9 tentativas)
- **Conhecimento de domÃ­nio** > aprendizado end-to-end em problemas simples
- **REINFORCE precisa mais dados** para convergir adequadamente
- **GPU implementada com sucesso** para escalabilidade futura

#### Q-Learning  
- **ConvergÃªncia robusta** independente de hiperparÃ¢metros
- **PolÃ­tica Ã³tima consistente** em todos os cenÃ¡rios
- **MDP bem-estruturado** com soluÃ§Ã£o clara
- **EstratÃ©gia emergente intuitiva:** explorar em high, conservar em low

### ContribuiÃ§Ãµes TÃ©cnicas

1. **ImplementaÃ§Ã£o Completa:** CÃ³digo modular e documentado
2. **Suporte GPU:** Sistema automÃ¡tico de detecÃ§Ã£o/otimizaÃ§Ã£o  
3. **AnÃ¡lise Comparativa:** AvaliaÃ§Ã£o rigorosa de diferentes abordagens
4. **Reprodutibilidade:** ConfiguraÃ§Ãµes completamente especificadas

### LimitaÃ§Ãµes Identificadas

- **REINFORCE:** Poucos episÃ³dios, falta de baseline para variÃ¢ncia
- **ComparaÃ§Ã£o:** AusÃªncia de outros algoritmos (A2C, PPO)
- **Escala:** Problema pequeno nÃ£o evidencia vantagens GPU

---

## ğŸ”® Trabalhos Futuros

### ExtensÃµes Promissoras

1. **Algoritmos AvanÃ§ados:** Actor-Critic, PPO, SAC
2. **OtimizaÃ§Ã£o AutomÃ¡tica:** Grid search, Bayesian optimization
3. **MDPs Complexos:** Estados contÃ­nuos, mÃºltiplos agentes
4. **Escalabilidade GPU:** Redes maiores, ambientes paralelos
5. **AplicaÃ§Ãµes Reais:** RobÃ³tica, jogos, controle industrial

### Melhorias TÃ©cnicas

- Implementar baseline para reduÃ§Ã£o de variÃ¢ncia
- Adicionar tÃ©cnicas de regularizaÃ§Ã£o avanÃ§adas
- Explorar arquiteturas de rede mais sofisticadas
- Desenvolver benchmarks mais desafiadores

---

## ğŸ“š ReferÃªncias CientÃ­ficas

1. **Williams, R. J.** (1992). "Simple statistical gradient-following algorithms for connectionist reinforcement learning." *Machine Learning*, 8(3-4), 229-256.

2. **Sutton, R. S. & Barto, A. G.** (2018). *Reinforcement Learning: An Introduction*, 2nd ed. MIT Press.

3. **Farama Foundation** (2022). "Gymnasium: A standard API for reinforcement learning." https://gymnasium.farama.org

4. **Paszke, A.** et al. (2019). "PyTorch: An imperative style, high-performance deep learning library." *Advances in Neural Information Processing Systems*.

---

## ğŸ› ï¸ Requisitos TÃ©cnicos

### Sistema MÃ­nimo
- Python 3.12+
- 4GB RAM  
- Processador multi-core

### Sistema Recomendado  
- Python 3.12+
- 8GB+ RAM
- GPU NVIDIA com CUDA 11.0+
- 4GB+ VRAM

### DependÃªncias Python
```txt
gymnasium>=1.2.2
torch>=2.0.0
numpy>=1.24.0
matplotlib>=3.7.0
```

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a **MIT License** - veja o arquivo [LICENSE](LICENSE) para detalhes.

---

## ğŸ¤ ContribuiÃ§Ãµes

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor:

1. **Fork** o projeto
2. Crie uma **branch** para sua feature (`git checkout -b feature/nova-funcionalidade`)
3. **Commit** suas mudanÃ§as (`git commit -am 'Adiciona nova funcionalidade'`)
4. **Push** para a branch (`git push origin feature/nova-funcionalidade`)  
5. Abra um **Pull Request**

---

## ğŸ“ Contato

**Diego Giovanni de AlcÃ¢ntara Vieira**
- ğŸ“§ Email: diego.vieira@ufam.edu.br
- ğŸ“ Programa de PÃ³s-GraduaÃ§Ã£o em Engenharia ElÃ©trica - UFAM
- ğŸ“ Manaus, Amazonas, Brasil

---

## â­ Agradecimentos

- **Universidade Federal do Amazonas (UFAM)**
- **Programa de PÃ³s-GraduaÃ§Ã£o em Engenharia ElÃ©trica**
- **Comunidade PyTorch e Gymnasium**
- **Desenvolvedores de Aprendizado por ReforÃ§o**

---

**ğŸ¯ Este projeto demonstra implementaÃ§Ãµes rigorosas de algoritmos fundamentais de Aprendizado por ReforÃ§o com infraestrutura computacional moderna, estabelecendo base sÃ³lida para pesquisas futuras em domÃ­nios de maior complexidade.**