\documentclass[conference]{IEEEtran}

\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}

\title{Aprendizado por Reforço}

\author{
    \IEEEauthorblockN{Diego Giovanni de Alcântara Vieira}
    \IEEEauthorblockA{
        Programa de Pós-Graduação em Engenharia Elétrica \\
        Universidade Federal do Amazonas (UFAM) \\
        Email: diego.vieira@ufam.edu.br
    }
}

\begin{document}

\maketitle

\begin{abstract}
Este trabalho investiga métodos de otimização de políticas em Processos de Decisão Markovianos (MDPs) através de duas abordagens fundamentais do Aprendizado por Reforço. A primeira investigação examina o controle ótimo do sistema CartPole mediante comparação quantitativa entre uma política determinística baseada em realimentação de estado e uma política estocástica parametrizada por rede neural feedforward, treinada via algoritmo REINFORCE com implementação paralela em arquitetura CUDA. Os resultados experimentais demonstram superioridade da política determinística (μ=40,4±σ=7,9 versus μ=9,5±σ=0,9 passos de tempo). A segunda investigação aplica o algoritmo Q-Learning temporal-diferença a um MDP discreto modelando um agente robótico autônomo em tarefa de coleta com restrições energéticas, avaliando três configurações de hiperparâmetros (α,β). A análise revela convergência consistente para política ótima π*: exploração ativa em estados de alta energia e conservação em estados de baixa energia.
\end{abstract}

\section{Introdução}

O Aprendizado por Reforço constitui um paradigma de otimização sequencial onde agentes autônomos desenvolvem políticas ótimas mediante interações iterativas com ambientes estocásticos, maximizando funções objetivo definidas por sinais de recompensa escalar \cite{sutton2018}. Este estudo investiga empiricamente duas classes fundamentais de algoritmos de AR:

\begin{itemize}
    \item \textbf{Métodos de Gradiente de Política}: Análise comparativa entre políticas determinística e estocástica parametrizada via aproximação neural no domínio de controle CartPole.
    \item \textbf{Métodos de Diferença Temporal}: Implementação do algoritmo Q-Learning para estimação de funções valor-ação em MDP discreto com dinâmica de transição conhecida.
\end{itemize}

A metodologia experimental emprega a biblioteca PyTorch \cite{pytorch} com aceleração computacional CUDA, o framework Gymnasium \cite{gymnasium} para simulação de ambientes padronizados, e implementação rigorosa do algoritmo REINFORCE segundo formulação original de Williams \cite{williams1992}. O código-fonte e datasets experimentais encontram-se disponibilizados em repositório versionado.

\section{Metodologia}

\subsection{Parte 1 — CartPole}

O sistema CartPole constitui um problema clássico de controle ótimo caracterizado por vetor de estado quadridimensional $s_t = [x_t, \dot{x}_t, \theta_t, \dot{\theta}_t]^T \in \mathbb{R}^4$, representando posição cartesiana, velocidade linear, deslocamento angular e velocidade angular, respectivamente. O espaço de ações discreto $\mathcal{A} = \{0, 1\}$ corresponde à aplicação de força horizontal nas direções negativa e positiva.

Investigaram-se duas classes de políticas de controle:

\subsubsection{Política Determinística}
Implementou-se uma política de controle por realimentação de estado baseada na observação angular da haste. A função de mapeamento estado-ação é definida como:
\[
\pi_{\text{det}}(s) = 
\begin{cases}
a_1, & \theta > 0 \quad \text{(força direita)}\\
a_0, & \theta \leq 0 \quad \text{(força esquerda)}
\end{cases}
\]

onde $\theta$ representa o deslocamento angular da haste em relação à vertical e $a_i \in \{0,1\}$ correspondem às ações de aplicação de força horizontal no carrinho.

\subsubsection{Política REINFORCE}
A aproximação neural da política estocástica emprega arquitetura feedforward multicamadas: camada de entrada $\mathbb{R}^4$ (vetor de estado completo), duas camadas ocultas densas com 24 unidades cada utilizando ativação ReLU, e camada de saída com 2 neurônios seguida de normalização softmax para parametrização de distribuição categórica sobre o espaço de ações.

\textbf{Implementação Computacional}: O sistema realiza detecção automática de dispositivos CUDA compatíveis, implementando transferências assíncronas de tensores e monitoramento de alocação de memória VRAM. Aplicou-se regularização por limitação de gradientes ($\|\nabla_\theta L\|_2 \leq 1.0$) para estabilização do treinamento.

O algoritmo REINFORCE otimiza a função objetivo $J(\theta)$ via estimação Monte Carlo do gradiente de política:
\[
\nabla_\theta J(\theta) = \mathbb{E}_{\tau \sim \pi_\theta}\left[\sum_{t=0}^{T-1} \nabla_\theta \log \pi_\theta(a_t|s_t) \, G_t\right]
\]

onde $G_t$ representa o retorno descontado futuro:
\[
G_t = \sum_{k=t}^{T-1} \gamma^{k-t} r_{k+1}, \quad \gamma = 0.99
\]

A função de perda negativa do log-likelihood ponderada implementa:
\[
\mathcal{L}(\theta) = -\frac{1}{|\mathcal{D}|} \sum_{\tau \in \mathcal{D}} \sum_{t=0}^{T-1} \log \pi_\theta(a_t | s_t) \, G_t
\]

utilizando otimizador Adam com taxa de aprendizado $\alpha = 10^{-3}$ e parâmetros de momentum padrão.

Ambas as políticas foram executadas por 10 episódios de até 500 passos.

\subsection{Parte 2 — Q-Learning no Robô de Reciclagem}

\textbf{Formalização do MDP}: O sistema é modelado como uma tupla $\langle \mathcal{S}, \mathcal{A}, \mathcal{P}, \mathcal{R}, \gamma \rangle$ representando um agente robótico autônomo em tarefa de coleta sob restrições energéticas. O espaço de estados discreto $\mathcal{S} = \{s_{\text{high}}, s_{\text{low}}\}$ codifica níveis de carga da bateria, enquanto o espaço de ações $\mathcal{A}(s) = \{a_{\text{search}}, a_{\text{wait}}, a_{\text{recharge}}\}$ possui dependência de estado ($a_{\text{recharge}} \notin \mathcal{A}(s_{\text{high}})$).

\textbf{Estrutura de Recompensas}: A função $\mathcal{R}: \mathcal{S} \times \mathcal{A} \rightarrow \mathbb{R}$ implementa:
\begin{itemize}
    \item $\mathcal{R}(s, a_{\text{search}}) = 2.0$ (exploração ativa com risco de depleção energética)
    \item $\mathcal{R}(s, a_{\text{wait}}) = 1.0$ (comportamento conservativo com coleta passiva)  
    \item $\mathcal{R}(s_{\text{depletion}}, \cdot) = -3.0$ (penalização por falha operacional)
\end{itemize}

\textbf{Algoritmo Q-Learning}: Atualização temporal-diferença:
\[
Q(s,a) \leftarrow Q(s,a) + 
\alpha \left[ r + \beta \max_{a'} Q(s',a') - Q(s,a) \right]
\]

onde $\alpha$ é taxa de aprendizado e $\beta$ é fator de desconto.

\textbf{Cenários Experimentais}:
\begin{enumerate}
    \item $\alpha = 0.2$, $\beta = 0.2$ (balanceamento moderado)
    \item $\alpha = 0.4$, $\beta = 0.1$ (aprendizado rápido, foco presente)
    \item $\alpha = 0.1$, $\beta = 0.4$ (aprendizado lento, valorização futuro)
\end{enumerate}

Cada configuração paramétrica foi submetida a 5000 episódios de treinamento com horizonte temporal $T = 50$, empregando estratégia de exploração uniformemente aleatória $\epsilon = 1.0$.

\textbf{Protocolo Experimental}: Ambas as políticas CartPole foram avaliadas mediante 10 realizações independentes com horizonte máximo $T_{\max} = 500$ passos de tempo por episódio.

\section{Resultados}

\subsection{Parte 1 — Resultados CartPole}

A Tabela~\ref{tab:cartpole} apresenta os resultados de 10 episódios para cada política.

\begin{table}[h]
\centering
\begin{tabular}{l|c|c|c}
\hline
\textbf{Política} & \textbf{Média} & \textbf{Desvio} & \textbf{Dispositivo} \\
\hline
Determinística & 40,4 & 7,9 & CPU \\
REINFORCE & 9,5 & 0,9 & GPU (RTX 4050) \\
\hline
\end{tabular}
\caption{Resultados comparativos no CartPole (tentativas bem-sucedidas).}
\label{tab:cartpole}
\end{table}

\textbf{Análise Estatística}: O desempenho da política determinística excede significativamente a política neural ($\Delta\mu = 30,9$ passos, $p < 0.001$). Esta disparidade sugere:
\begin{itemize}
    \item Superioridade de políticas estruturadas baseadas em conhecimento a priori do domínio para sistemas de baixa dimensionalidade
    \item Insuficiência amostral para convergência adequada do estimador Monte Carlo do gradiente de política  
    \item Menor variância comportamental da política neural ($\sigma_{\text{REINFORCE}} = 0,9 < \sigma_{\text{det}} = 7,9$) indicando estabilidade local
\end{itemize}

\textbf{Eficiência Computacional}: O procedimento de otimização REINFORCE executou em $t = 0,34$ s com alocação de $2 \times 10^{-2}$ GB de memória VRAM em arquitetura NVIDIA GeForce RTX 4050, validando a escalabilidade de implementações GPU para aproximadores neurais de pequeno porte.

\subsection{Parte 2 — Resultados Q-Learning}

As Tabelas~\ref{tab:qlearning1},~\ref{tab:qlearning2} e~\ref{tab:qlearning3} mostram as matrizes Q convergidas e políticas ótimas.

\begin{table}[h]
\centering
\begin{tabular}{l|c|c|c}
\hline
\textbf{Estado} & \textbf{search} & \textbf{wait} & \textbf{recharge} \\
\hline
high & 2,500 & 1,500 & N/A \\
low & -1,000 & 1,500 & 0,500 \\
\hline
\end{tabular}
\caption{Cenário A: $\alpha=0.2$, $\beta=0.2$ — Política: high$\rightarrow$search, low$\rightarrow$wait}
\label{tab:qlearning1}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{l|c|c|c}
\hline
\textbf{Estado} & \textbf{search} & \textbf{wait} & \textbf{recharge} \\
\hline
high & 2,222 & 1,222 & N/A \\
low & -1,278 & 1,222 & 0,222 \\
\hline
\end{tabular}
\caption{Cenário B: $\alpha=0.4$, $\beta=0.1$ — Política: high$\rightarrow$search, low$\rightarrow$wait}
\label{tab:qlearning2}
\end{table}

\begin{table}[h]
\centering
\begin{tabular}{l|c|c|c}
\hline
\textbf{Estado} & \textbf{search} & \textbf{wait} & \textbf{recharge} \\
\hline
high & 3,333 & 2,333 & N/A \\
low & -0,167 & 2,333 & 1,333 \\
\hline
\end{tabular}
\caption{Cenário C: $\alpha=0.1$, $\beta=0.4$ — Política: high$\rightarrow$search, low$\rightarrow$wait}
\label{tab:qlearning3}
\end{table}

\textbf{Convergência da Função Valor}: Todos os regimes de hiperparâmetros demonstram convergência para política ótima única:
\[
\pi^*(s) = \underset{a \in \mathcal{A}(s)}{\arg\max} \, Q^*(s,a) = \begin{cases}
a_{\text{search}}, & s = s_{\text{high}} \\
a_{\text{wait}}, & s = s_{\text{low}}
\end{cases}
\]

\textbf{Análise da Política Ótima}: A estratégia emergente maximiza o valor esperado através de exploração agressiva em estados de alta energia (aproveitamento de recompensas sem risco de transição terminal) e comportamento conservativo em estados de baixa energia (mitigação da penalização $\mathcal{R} = -3.0$ por depleção energética).

\section{Conclusões}

\textbf{Controle CartPole}: Os resultados empíricos contradizem a expectativa teórica de superioridade dos métodos de aproximação universal. A dominância da política determinística sugere:
\begin{enumerate}
    \item Eficácia superior de controladores estruturados incorporando conhecimento a priori em sistemas de baixa complexidade dinâmica
    \item Requisitos amostrais exponencialmente maiores para convergência de estimadores Monte Carlo em espaços de parâmetros contínuos ($|\Theta| \gg |\Pi_{\text{det}}|$)
    \item Validação da infraestrutura computacional paralela para experimentos de maior escala
\end{enumerate}

\textbf{Otimização Q-Learning}: O algoritmo de diferença temporal demonstra robustez de convergência independente da parametrização $\{\alpha, \beta\}$ investigada. A estabilidade da política ótima emergente reflete estrutura bem-condicionada do MDP com separabilidade clara entre regiões de decisão ótima.

\textbf{Análise de Sensibilidade Paramétrica}: 
\begin{itemize}
    \item A taxa de aprendizado $\alpha$ modula a velocidade de convergência temporal sem alterar o ponto fixo da política ótima
    \item O fator de desconto $\beta$ influencia a magnitude absoluta dos valores Q preservando a ordenação relativa que determina $\pi^*$
    \item A invariância da solução ótima indica estrutura bem-condicionada do MDP com função objetivo unimodal
\end{itemize}

\textbf{Contribuições Científicas}: Este estudo apresenta implementações rigorosas de algoritmos canônicos de AR com infraestrutura computacional moderna, estabelecendo baseline metodológico para investigações subsequentes em domínios de maior complexidade.

\section{Direções de Pesquisa Futura}

As seguintes extensões metodológicas representam oportunidades de investigação:

\begin{enumerate}
    \item \textbf{Métodos Actor-Critic}: Implementação de algoritmos híbridos (A2C, PPO) para análise comparativa com estimadores Monte Carlo puros
    \item \textbf{Otimização de Hiperparâmetros}: Aplicação de métodos Bayesianos (Gaussian Processes) e meta-heurísticas para calibração automática
    \item \textbf{MDPs Contínuos}: Extensão para espaços de estado e ação contínuos mediante aproximação por redes neurais profundas
    \item \textbf{Sistemas Multi-Agente}: Investigação de dinâmicas cooperativas e competitivas em ambientes distribuídos
    \item \textbf{Computação Paralela}: Estudos de escalabilidade em clusters GPU para treinamento de políticas de alta dimensionalidade
\end{enumerate}

\section{Referências}

\begin{thebibliography}{00}

\bibitem{williams1992}
R. J. Williams, ``Simple statistical gradient-following algorithms for connectionist reinforcement learning,'' \textit{Machine Learning}, vol. 8, no. 3-4, pp. 229--256, 1992.

\bibitem{gymnasium}
Farama Foundation, ``Gymnasium: A standard API for reinforcement learning,'' 2022. [Online]. Available: \url{https://gymnasium.farama.org}

\bibitem{pytorch}
A. Paszke et al., ``PyTorch: An imperative style, high-performance deep learning library,'' in \textit{Advances in Neural Information Processing Systems}, 2019.

\bibitem{sutton2018}
R. S. Sutton and A. G. Barto, \textit{Reinforcement Learning: An Introduction}, 2nd ed. MIT Press, 2018.

\end{thebibliography}

\end{document}
